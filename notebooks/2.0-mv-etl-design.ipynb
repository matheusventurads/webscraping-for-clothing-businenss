{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3229bf7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T15:50:44.596167Z",
     "start_time": "2022-11-29T15:50:44.588154Z"
    }
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eed826c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T15:50:51.676247Z",
     "start_time": "2022-11-29T15:50:50.858003Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from datetime   import datetime\n",
    "from bs4        import BeautifulSoup\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57a2d1",
   "metadata": {},
   "source": [
    "# ETL for python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc80886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collection\n",
    "\n",
    "# parameters\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:106.0) Gecko/20100101 Firefox/106.0'}\n",
    "\n",
    "# URL\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "# requesto to URL\n",
    "page = requests.get(url, headers=headers)\n",
    "\n",
    "# BeautifuylSoup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# getting number of products\n",
    "total_item = soup.find_all('h2', class_='load-more-heading')[0].get('data-total')\n",
    "total_item\n",
    "\n",
    "# calculating number of pages\n",
    "page_number = np.ceil(int(total_item)/36)\n",
    "page_number\n",
    "\n",
    "# generating url\n",
    "url02 = url + '?page-size=' + str(int(page_number)*36)\n",
    "url02\n",
    "\n",
    "# request to new URL\n",
    "page = requests.get(url02, headers=headers)\n",
    "\n",
    "# BeautifulSoup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# product details\n",
    "products = soup.find('ul', class_='products-listing small')\n",
    "product_list = products.find_all('article', class_='hm-product-item')\n",
    "\n",
    "# product id\n",
    "product_id = [p.get('data-articlecode') for p in product_list]\n",
    "\n",
    "# product category\n",
    "product_category = [p.get('data-category') for p in product_list]\n",
    "\n",
    "# product name\n",
    "product_list = products.find_all('a', class_='link')\n",
    "product_name = [p.get_text() for p in product_list]\n",
    "\n",
    "# price\n",
    "product_list = products.find_all('span', class_='price regular')\n",
    "product_price = [p.get_text() for p in product_list]\n",
    "\n",
    "# creating dataset\n",
    "data = pd.DataFrame([product_id, product_category, product_name, product_price]).T\n",
    "data.columns = ['product_id', 'product_category', 'product_name', 'product_price']\n",
    "\n",
    "# scrapy datetime\n",
    "data['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# empty dataframe for append\n",
    "df_color = pd.DataFrame()\n",
    "df_composition = pd.DataFrame()\n",
    "\n",
    "# iteration for each id product\n",
    "for code in data['product_id']:\n",
    "    url02 = 'https://www2.hm.com/en_us/productpage.' + str(code) + '.html'\n",
    "\n",
    "    page = requests.get(url02, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # Product Color\n",
    "    color_name = soup.find('a', class_='filter-option miniature active').get('data-color')\n",
    "\n",
    "    # product id\n",
    "    product_code = soup.find('a', class_='filter-option miniature active').get('data-articlecode')\n",
    "\n",
    "    aux1 = pd.DataFrame({'product_id': product_code, 'color_name': color_name}, index=[0])\n",
    "    df_color = pd.concat([df_color, aux1])\n",
    "\n",
    "    # Product Composition \n",
    "    product_composition_list = soup.find_all('div', class_='details-attributes-list-item')\n",
    "\n",
    "    product_composition = [list(filter(None, p.get_text().split('\\n'))) for p in product_composition_list]\n",
    "    \n",
    "    composition = pd.DataFrame(product_composition).T\n",
    "\n",
    "    # rename dataframe\n",
    "    composition.columns = composition.iloc[0]\n",
    "\n",
    "    # delete first row\n",
    "    composition['Art. No.'] = composition['Art. No.'].fillna(method='ffill')\n",
    "    composition = composition.iloc[1:]\n",
    "\n",
    "    composition_aux = composition.fillna('').groupby(['Art. No.'], as_index=False).sum()\n",
    "\n",
    "    df_composition = pd.concat([df_composition, composition_aux], axis=0)\n",
    "    \n",
    "df_composition = df_composition[['Art. No.', 'Fit', 'Size', 'Composition', 'Additional material information']]\n",
    "df_composition.rename(columns={'Art. No.': 'product_id', 'Fit': 'fit', 'Size': 'size', 'Composition':\n",
    "                               'composition', 'Additional material information': 'additional_material'},\n",
    "                               inplace=True)\n",
    "\n",
    "# merging the dataframes\n",
    "df_details = pd.merge(df_color, df_composition, how='left', on='product_id')\n",
    "\n",
    "data = pd.merge(data, df_details, how='left', on='product_id')\n",
    "\n",
    "# Data manipulation with regex\n",
    "# product_name\n",
    "data['product_name'] = data['product_name'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "\n",
    "# product_price\n",
    "data['product_price'] = data['product_price'].apply(lambda x: x.replace ('$ ', '')).astype(float)\n",
    "\n",
    "# scrapy_datetime\n",
    "data['scrapy_datetime'] = pd.to_datetime(data['scrapy_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# color_name\n",
    "data['color_name'] = data['color_name'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "\n",
    "# Fit\n",
    "data['fit'] = data['fit'].apply(lambda x: x.replace(' ', '_').replace('/', '_').lower())\n",
    "\n",
    "# Composition - Shell\n",
    "data['shell_composition'] = np.NaN\n",
    "\n",
    "for index, line in data.iterrows():\n",
    "    # since there is no pattern, different methods are implemented\n",
    "    if re.match('Shell:.(.+%)\\w', line['composition']):\n",
    "        data.loc[index, 'shell_composition'] = re.match('Shell:.(.+%)\\w', line['composition']).group(1)\n",
    "    elif re.match('(.+)%\\w', line['composition']):\n",
    "        data.loc[index, 'shell_composition'] = re.match('(.+)%\\w', line['composition']).group(1)\n",
    "    else:\n",
    "        data.loc[index, 'shell_composition'] = line['composition']\n",
    "\n",
    "# Composition - Lining\n",
    "for index, line in data.iterrows():\n",
    "    if re.search('Pock.+: (.+)', line['composition']):\n",
    "        data.loc[index, 'pocket_lining_composition'] = re.search('Pock.+: (.+)', line['composition']).group(1)\n",
    "\n",
    "# size\n",
    "data['size_number'] = np.NaN\n",
    "data['leg_lenght'] = np.NaN\n",
    "data['circumference'] = np.NaN\n",
    "\n",
    "for index, line in data.iterrows():\n",
    "    # look for lines with text for extraction\n",
    "    if pd.notnull(line['size']):\n",
    "        if re.search('Length: (.{1,4}) cm', line['size']):\n",
    "            data.loc[index, 'leg_lenght'] = re.search('Length: (.{1,4}) cm', line['size']).group(1)\n",
    "        if re.search('Circumference: (.{1,4}) cm', line['size']):\n",
    "            data.loc[index, 'circumference'] = re.search('Circumference: (.{1,4}) cm', line['size']).group(1)\n",
    "\n",
    "        if re.search('\\(Size (.+)\\)\\w', line['size']):\n",
    "            data.loc[index, 'size_number'] = re.search('\\(Size (.+)\\)\\w', line['size']).group(1)\n",
    "        elif re.search('\\(Size (.+)\\)', line['size']):\n",
    "            data.loc[index, 'size_number'] = re.search('\\(Size (.+)\\)', line['size']).group(1)\n",
    "\n",
    "# Spliting shell composition\n",
    "\n",
    "# Cotton, Spandex, Polyester, Elastomultiester, Rayon, Lyocell\n",
    "data['cotton'] = data['shell_composition'].apply(lambda x: int(re.search('Cotton (\\d{1,3})', x).group(1))/100\n",
    "                                                 if re.search('Cotton (\\d{1,3})', x) else np.NaN)\n",
    "\n",
    "data['spandex'] = data['shell_composition'].apply(lambda x: int(re.search('Spandex (\\d{1,3})',x).group(1))/100\n",
    "                                                  if re.search('Spandex (\\d{1,3})', x) else np.NaN)\n",
    "\n",
    "data['polyester'] = data['shell_composition'].apply(lambda x:\n",
    "                                                    int(re.search('Polyester (\\d{1,3})',x).group(1))/100\n",
    "                                                    if re.search('Polyester (\\d{1,3})', x) else np.NaN)\n",
    "\n",
    "data['elastomultiester'] = data['shell_composition'].apply(lambda x:\n",
    "                                                           int(re.search('Elastomultiester (\\d{1,3})',\n",
    "                                                                         x).group(1))/100 \n",
    "                                                           if re.search('Elastomultiester (\\d{1,3})', x) else\n",
    "                                                           np.NaN)\n",
    "\n",
    "data['rayon'] = data['shell_composition'].apply(lambda x: int(re.search('Rayon (\\d{1,3})', x).group(1))/100\n",
    "                                                if re.search('Rayon (\\d{1,3})', x) else np.NaN)\n",
    "\n",
    "data['lyocell'] = data['shell_composition'].apply(lambda x: int(re.search('Lyocell (\\d{1,3})',x).group(1))/100\n",
    "                                                  if re.search('Lyocell (\\d{1,3})', x) else np.NaN)\n",
    "\n",
    "data = data.drop(['size', 'composition', 'shell_composition'], axis=1)\n",
    "\n",
    "for index, line in data.iterrows():\n",
    "    # check for null values\n",
    "    if pd.notnull(line['pocket_lining_composition']):\n",
    "        if re.search('Cotton (\\d{1,3})', line['pocket_lining_composition']):\n",
    "            data.loc[index, 'cotton_pocket'] = int(re.search('Cotton (\\d{1,3})',\n",
    "                                                         line['pocket_lining_composition']).group(1))/100\n",
    "        \n",
    "        if re.search('Polyester (\\d{1,3})', line['pocket_lining_composition']):\n",
    "            data.loc[index, 'polyester_pocket'] = int(re.search('Polyester (\\d{1,3})',\n",
    "                                                         line['pocket_lining_composition']).group(1))/100\n",
    "    if pd.notnull(line['additional_material']):\n",
    "        if re.search('cotton (\\d{1,3})', line['additional_material']):\n",
    "            data.loc[index, 'recycled_cotton'] = int(re.search('cotton (\\d{1,3})',\n",
    "                                                        line['additional_material']).group(1))/100\n",
    "        \n",
    "        if re.search('polyester (\\d{1,3})', line['additional_material']):\n",
    "            data.loc[index, 'recycled_polyester'] = int(re.search('cotton (\\d{1,3})',\n",
    "                                                        line['additional_material']).group(1))/100\n",
    "            \n",
    "data = data.drop(['pocket_lining_composition', 'additional_material'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73611a3",
   "metadata": {},
   "source": [
    "# Inserting data to DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e12a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-29T15:50:57.723879Z",
     "start_time": "2022-11-29T15:50:57.703439Z"
    }
   },
   "outputs": [],
   "source": [
    "# query for the table creation\n",
    "query_showroom_schema = \"\"\"\n",
    "    CREATE TABLE showroom (\n",
    "        product_id         INTEGER,\n",
    "        product_category   TEXT,\n",
    "        product_name       TEXT,\n",
    "        product_price      REAL,\n",
    "        scrapy_datetime    TEXT,\n",
    "        color_name         TEXT,\n",
    "        fit                TEXT,\n",
    "        size_number        TEXT,\n",
    "        leg_lenght         TEXT, \n",
    "        circumference      TEXT,\n",
    "        cotton             REAL,\n",
    "        spandex            REAL,\n",
    "        polyester          REAL,\n",
    "        elastomultiester   REAL,  \n",
    "        rayon              REAL,\n",
    "        lyocell            REAL,\n",
    "        cotton_pocket      REAL,\n",
    "        polyester_pocket   REAL,   \n",
    "        recycled_cotton    REAL,\n",
    "        recycled_polyester REAL\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect('database_hm.sqlite')\n",
    "\n",
    "# plan the execution\n",
    "cursor = conn.execute(query_showroom_schema)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4fe6825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-22T13:03:38.143024Z",
     "start_time": "2022-11-22T13:03:38.119794Z"
    }
   },
   "outputs": [],
   "source": [
    "# connecting to database\n",
    "engine = create_engine('sqlite:///database_hm.sqlite', echo=False)\n",
    "\n",
    "# inserting data into database\n",
    "with engine.connect() as connection:\n",
    "    data.to_sql('showroom', con=connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddde656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
